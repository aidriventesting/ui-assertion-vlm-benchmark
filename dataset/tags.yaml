# UI Assertion Benchmark - Tag Taxonomy (v2)
# Clean separation: assertion_type (WHAT) vs cognitive_level (HOW COMPLEX)

# =============================================================================
# ASSERTION TYPES — What you're verifying (core operation)
# =============================================================================
assertion_types:
  presence: "Element exists in view"
  absence: "Element does NOT exist"
  count: "Number of elements matches expected"
  text_match: "Text matches (mode: exact/normalized/semantic)"
  order: "Items sorted correctly (key: alpha/numeric/time/custom)"
  layout_relation: "Spatial relationship (left-of, aligned, above)"
  state: "Widget state (toggle on/off, disabled, visible)"
  consistency: "Cross-field validation (rule: arithmetic/format/business)"
  a11y_visual: "Visual accessibility (contrast, focus, readability)"

# =============================================================================
# MODE SUB-PARAMETERS — Refines the assertion type
# =============================================================================
modes:
  text_match_mode:
    exact: "Character-for-character match"
    normalized: "Match after case/whitespace normalization"
    semantic_closed: "Meaning match within known synonym set (L2)"
    semantic_open: "Open interpretation of meaning/intent (L3)"

  count_scope:
    raw: "Count all visible objects of type (L1)"
    filtered: "Count only items matching attribute (L2)"
    cross_check: "Compare count from two sources (L2+)"

  order_key:
    alpha: "Alphabetical sorting"
    numeric: "Numeric/price sorting"
    time: "Chronological sorting"
    custom: "Domain-specific order rule (often L3)"

  rule_kind:
    arithmetic: "Math check (sum, multiplication)"
    format: "Format validation (date, phone, email)"
    business: "Business rule requiring domain knowledge (L3)"

# =============================================================================
# COGNITIVE LEVELS — What operations are minimally required?
# =============================================================================
cognitive_levels:
  L1:
    name: "Perceptive"
    test: "1 zone, detect/read/count, no comparison, no rule"
    examples:
      - "Is button X visible?"
      - "What text is shown in the header?"
      - "How many cards are on screen?"
    primitives: ["detection", "OCR", "counting"]

  L2:
    name: "Relational"
    test: "Compare A vs B, verify constraint, spatial relation, or filtered operation"
    examples:
      - "Is list sorted alphabetically?"
      - "Is button A to the left of button B?"
      - "Count only 'direct' trips (filtered count)"
      - "Does badge count match list length? (cross-check)"
    primitives: ["comparison", "spatial reasoning", "filtering", "constraint check"]

  L3:
    name: "Semantic/Intent"
    test: "Interpret meaning, understand intent, or apply business rule not directly observable"
    examples:
      - "Does this message indicate an error? (intent interpretation)"
      - "Does the journey match the search criteria? (business rule)"
      - "Is this screen compliant with regulation X?"
    primitives: ["semantic understanding", "domain knowledge", "intent inference"]
    warning: "Keep anchored to observables — avoid subjective UX philosophy"

# =============================================================================
# DIFFICULTY DRIVERS — Why it's hard (independent of cognitive level)
# =============================================================================
difficulty_drivers:
  near_miss: "Subtle difference (typo, similar icon, almost-correct order)"
  small_text: "Text requires zoom to read"
  occluded: "Element partially hidden"
  partial_visibility: "Element at viewport edge"
  low_contrast: "Low color contrast / dark mode"
  busy_screen: "Many competing elements"
  confusable_icons: "Similar looking icons"
  dynamic_content: "Live/changing data"
  localization: "Language/format variations"
  scroll_required: "Info below fold"

# =============================================================================
# SEVERITY — Business impact
# =============================================================================
severity:
  low: "Cosmetic issue"
  med: "Degraded UX but functional"
  high: "Blocks flow or critical error"

# =============================================================================
# TAGGING PROCEDURE (follow in order)
# =============================================================================
# 1. assertion_type — What operation? (presence/text_match/count/...)
# 2. mode/scope — Refine if applicable (exact vs semantic, raw vs filtered)
# 3. cognitive_level — Test: 1 zone no compare → L1, compare/constrain → L2, intent/rule → L3
# 4. difficulty_drivers — What makes it hard? (near_miss, small_text, ...)
# 5. severity — Business impact (low/med/high)
